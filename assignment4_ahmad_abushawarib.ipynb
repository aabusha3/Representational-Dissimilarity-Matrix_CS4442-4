{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/vortexkol/alexnet-cnn-architecture-on-tensorflow-beginner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Use one of the deep learning frameworks (Pytorch, Tensorflow, Keras, ...) and\n",
    "load the pre-trained AlexNet model. Input these 156 images to the pre-trained AlexNet\n",
    "model and extract feature maps/activations from Conv 1, 2, 3, 4, 5, fc6, and fc7 layers.\n",
    "Vectorize the activations corresponding to each image. You should have a vector of\n",
    "activations per image per layer mentioned above. (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.alexnet(weights='DEFAULT')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgOutputs = []\n",
    "path = \"./Image Set\"\n",
    "for file in os.listdir(path):\n",
    "    img=os.path.join(path,file)\n",
    "\n",
    "    model = create_feature_extractor(model, \n",
    "                    {\"features.0\":\"conv1\",\"features.3\":\"conv2\",\"features.6\":\"conv3\",\"features.8\":\"conv4\",\"features.10\":\"conv5\",\n",
    "                    \"classifier.1\":\"fc6\",\"classifier.4\":\"fc7\"})\n",
    "\n",
    "    input_image = Image.open(img)\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    input_tensor = preprocess(input_image)\n",
    "    input_batch = input_tensor.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        out = model(input_batch)\n",
    "\n",
    "    for key in out:\n",
    "        out[key]=out[key].view(out[key].size(0), -1)\n",
    "\n",
    "    imgOutputs.append(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Create a representational dissimilarity matrix (RDM) which is 156 x156 matrix,\n",
    "each row and column in this matrix is indexed by one of the images in the image set\n",
    "and each element in the matrix is the Euclidean distance between the activation vectors\n",
    "of the corresponding images you extracted in Step 1. (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def generateRDM(layer):\n",
    "    RDM = [ [0]*len(imgOutputs) for l in range(len(imgOutputs)) ]\n",
    "    for i,img1 in enumerate(imgOutputs):\n",
    "        for j,img2 in enumerate(imgOutputs):\n",
    "            RDM[i][j] = norm(img1[layer]-img2[layer])\n",
    "    return RDM\n",
    "\n",
    "RDMS = { layer:generateRDM(layer) for layer in imgOutputs[0] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([193600])\n",
      "torch.Size([193600])\n",
      "478069.2\n"
     ]
    }
   ],
   "source": [
    "# for img in imgOutputs:\n",
    "#     for layer in img:\n",
    "#         print(layer.shape)\n",
    "a=imgOutputs[0][0]\n",
    "b=imgOutputs[1][0]\n",
    "print(a.shape)\n",
    "# print([i for i in a.shape])\n",
    "print(b.shape)\n",
    "c=sum(((a-b)**2)).numpy()\n",
    "# c=sum(((a-b)**2).reshape(193600))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Plot the RDM for each layer (Conv 1, 2, 3, 4, 5, fc6, fc7), and their\n",
    "corresponding multidimensional scaling (MDS) visualization in 2 Dimension. The class\n",
    "labels you should use for the MDS plotting includes images 1 to 28 are Animals, 29-64\n",
    "are Objects, 65 to 100 are scenes, 101 to 124 are human activities, 125 to 156 are\n",
    "faces. (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please write a short report presenting and discussing the results of this assignment. (20\n",
    "points)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
